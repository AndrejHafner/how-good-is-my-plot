{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "plot_classificiation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlbR6aq_oNoX",
        "outputId": "c8381c1a-8ffb-4364-cae5-185bc022e6c4"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avPVrHIgYYxD",
        "outputId": "762207c1-8197-4be0-e219-7c67ef775e09"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCzVPt_TqXhO",
        "outputId": "1ed97657-a984-426a-fbe4-7dcd9466a205"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWXhmQEhYtXw"
      },
      "source": [
        "from drive.MyDrive.how_good_is_my_plot.fine_tune_for_plots import train_model, initialize_model\n",
        "\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "# Data directory on which you want to train the model\n",
        "data_dir = \"drive/MyDrive/how_good_is_my_plot/data\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 8\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 48\n",
        "\n",
        "# Number of epochs to train for  -> can leave this on 50 - the model with best validation loss will be saved.\n",
        "num_epochs = 50\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(num_classes, use_pretrained=True)\n",
        "\n",
        "# Resizing and normalizing data\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((input_size, input_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((input_size, input_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {\n",
        "    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=3) for x in\n",
        "    ['train', 'val']}\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "params_to_update = model_ft.parameters()\n",
        "\n",
        "# you can change learning rate here\n",
        "learning_rate = 0.0001\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=learning_rate)\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, device, num_epochs=num_epochs)\n",
        "\n",
        "torch.save(model_ft.state_dict(), f\"drive/MyDrive/how_good_is_my_plot/cnn_weights/resnet101_phase3.pth\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}