# John Doe's Data Science Project Competition journal

## February 2020 (8h)

* 26. (2h): Watched XGBoost tutorials. I think XGBoost will be the best machine learning algorithm for our project.
* 27. (2h): XGBoost implementation. I am using XGBoost from the xgboost python library.
* 28. (4h): Data preparation for XGBoost and test runs. Before running initial XGBoost test I had to prepare data in an appropriate format. First test runs look promising!

## March 2020 (2h)

* 3. (2h): XGBoost parameter tunning. I tried to find the combination of parameter values that gives me the best results. I changed the values of the following parameters:

  * eta (learning rate),
  * gamma (minimum loss reduction required to make a further partition on a leaf node of the tree),
  * max_depth (maximum depth of a tree).

* 4. (2h): ...

## April 2020 ([total hours for March])

...

## May 2020 ([total hours for May])

...

## June 2020 ([total hours for June])

...

## Total: [total sum of hours]
